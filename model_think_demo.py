"""
model.invoke æ­é… think_tool ä½¿ç”¨ç¯„ä¾‹ - çµæœè¼¸å‡º
"""
from src.app.tools.mcp_tools import think_tool
from src.app.config import get_llm_with_structured_output
import sys
import asyncio
from datetime import datetime
from langchain_core.messages import HumanMessage

# æ·»åŠ å°ˆæ¡ˆè·¯å¾‘
sys.path.append(
    'C:\\Users\\berto\\Desktop\\capstone project\\my_langgraph_agent')


async def demonstrate_model_think_integration():
    """æ¼”ç¤º model.invoke èˆ‡ think_tool çš„æ•´åˆä½¿ç”¨"""

    print("ğŸš€ model.invoke æ­é… think_tool ä½¿ç”¨ç¯„ä¾‹")
    print("=" * 60)

    # ç¯„ä¾‹ 1: åŸºæœ¬æ•´åˆ
    print("\nğŸ“ ç¯„ä¾‹ 1: åŸºæœ¬æ•´åˆ - äººå·¥æ™ºæ…§æ‡‰ç”¨åˆ†æ")
    print("-" * 50)

    try:
        # æ­¥é©Ÿ 1: æ¨¡å‹èª¿ç”¨
        model = get_llm_with_structured_output("gemini-2.5-pro")
        prompt = "è«‹åˆ†æäººå·¥æ™ºæ…§åœ¨é†«ç™‚é ˜åŸŸçš„ä¸»è¦æ‡‰ç”¨å’Œç™¼å±•è¶¨å‹¢"

        response = await model.ainvoke([HumanMessage(content=prompt)])
        print(f"âœ… æ¨¡å‹å›æ‡‰: {len(response.content)} å­—ç¬¦")

        # æ­¥é©Ÿ 2: ä½¿ç”¨ think_tool åæ€
        reflection = f"""
åˆ†æçµæœè©•ä¼°ï¼š
- å›æ‡‰é•·åº¦: {len(response.content)} å­—ç¬¦
- å…§å®¹æ¶µè“‹: é†«ç™‚AIæ‡‰ç”¨å’Œè¶¨å‹¢
- çµæ§‹å®Œæ•´æ€§: åŒ…å«ä¸»è¦æ‡‰ç”¨é ˜åŸŸ

å“è³ªè©•ä¼°ï¼š
- è³‡è¨Šå®Œæ•´æ€§: æ¶µè“‹ä¸»è¦æ‡‰ç”¨
- å¯¦ç”¨æ€§: æä¾›å¯¦ç”¨ä¿¡æ¯
- å°ˆæ¥­æ€§: æŠ€è¡“åˆ†ææº–ç¢º

æ”¹é€²å»ºè­°ï¼š
- å¯ä»¥æ·»åŠ æ›´å¤šå…·é«”æ¡ˆä¾‹
- éœ€è¦è£œå……çµ±è¨ˆæ•¸æ“š
- å¯ä»¥å¢åŠ é¢¨éšªåˆ†æ
"""

        think_result = think_tool.invoke({"reflection": reflection})
        print(f"âœ… åæ€å®Œæˆ: {think_result[:50]}...")

        # æ­¥é©Ÿ 3: åŸºæ–¼åæ€çš„æ”¹é€²
        improvement_prompt = f"""
åŸºæ–¼ä»¥ä¸‹åˆ†æå’Œåæ€ï¼Œæä¾›æ”¹é€²å»ºè­°ï¼š

åŸå§‹åˆ†æï¼š
{response.content[:300]}...

åæ€è¦é»ï¼š
{reflection}

è«‹æä¾›ï¼š
1. å…·é«”çš„æ”¹é€²å»ºè­°
2. éœ€è¦è£œå……çš„å…§å®¹
3. ä¸‹ä¸€æ­¥è¡Œå‹•è¨ˆåŠƒ
"""

        improvement = await model.ainvoke([HumanMessage(content=improvement_prompt)])
        print(f"âœ… æ”¹é€²å»ºè­°: {len(improvement.content)} å­—ç¬¦")

        example_1_result = {
            "original_length": len(response.content),
            "reflection_length": len(reflection),
            "improvement_length": len(improvement.content),
            "total_length": len(response.content) + len(improvement.content)
        }

    except Exception as e:
        print(f"âŒ ç¯„ä¾‹ 1 å¤±æ•—: {e}")
        example_1_result = None

    # ç¯„ä¾‹ 2: çµæ§‹åŒ–è¼¸å‡ºæ•´åˆ
    print("\nğŸ“ ç¯„ä¾‹ 2: çµæ§‹åŒ–è¼¸å‡ºæ•´åˆ - ç ”ç©¶è¨ˆåŠƒç”Ÿæˆ")
    print("-" * 50)

    try:
        from src.app.state.research_state import ClarifyWithUser

        # æ­¥é©Ÿ 1: çµæ§‹åŒ–æ¾„æ¸…
        structured_model = get_llm_with_structured_output(
            "gemini-2.5-pro", ClarifyWithUser)
        clarify_prompt = "æˆ‘æƒ³äº†è§£å€å¡ŠéˆæŠ€è¡“çš„ç™¼å±•ç¾ç‹€"

        clarify_response = await structured_model.ainvoke([HumanMessage(content=clarify_prompt)])
        print(f"âœ… æ¾„æ¸…å•é¡Œ: {clarify_response.question[:50]}...")
        print(f"âœ… é¸é …æ•¸é‡: {len(clarify_response.options)}")

        # æ­¥é©Ÿ 2: æ¾„æ¸…åæ€
        clarify_reflection = f"""
æ¾„æ¸…åˆ†æï¼š
- å•é¡Œé‡å°æ€§: å¼·
- é¸é …æ•¸é‡: {len(clarify_response.options)}
- é¸é …å…§å®¹: {clarify_response.options}

å“è³ªè©•ä¼°ï¼š
- å•é¡Œæ¸…æ™°åº¦: é«˜
- é¸é …ç›¸é—œæ€§: å¼·
- ç”¨æˆ¶å¼•å°æ€§: å¥½

æˆ°ç•¥æ±ºç­–ï¼š
- å»ºè­°ç”¨æˆ¶é¸æ“‡æœ€æ„Ÿèˆˆè¶£çš„é¸é …
- å¯ä»¥åŸºæ–¼é¸é …é€²è¡Œæ·±åº¦ç ”ç©¶
- éœ€è¦é€²ä¸€æ­¥æ¾„æ¸…å…·é«”éœ€æ±‚
"""

        clarify_think = think_tool.invoke({"reflection": clarify_reflection})
        print(f"âœ… æ¾„æ¸…åæ€å®Œæˆ")

        example_2_result = {
            "question_length": len(clarify_response.question),
            "options_count": len(clarify_response.options),
            "reflection_length": len(clarify_reflection)
        }

    except Exception as e:
        print(f"âŒ ç¯„ä¾‹ 2 å¤±æ•—: {e}")
        example_2_result = None

    # ç¯„ä¾‹ 3: å¤šè¼ªåæ€å·¥ä½œæµç¨‹
    print("\nğŸ“ ç¯„ä¾‹ 3: å¤šè¼ªåæ€å·¥ä½œæµç¨‹ - æ·±åº¦åˆ†æ")
    print("-" * 50)

    try:
        # åˆå§‹åˆ†æ
        initial_prompt = "è«‹åˆ†æé‡å­è¨ˆç®—çš„æŠ€è¡“åŸç†å’Œæ‡‰ç”¨å‰æ™¯"
        initial_response = await model.ainvoke([HumanMessage(content=initial_prompt)])
        print(f"âœ… åˆå§‹åˆ†æ: {len(initial_response.content)} å­—ç¬¦")

        # ç¬¬ä¸€è¼ªåæ€
        first_reflection = f"""
åˆå§‹åˆ†æè©•ä¼°ï¼š
- åˆ†æé•·åº¦: {len(initial_response.content)} å­—ç¬¦
- å…§å®¹æ·±åº¦: åŒ…å«æŠ€è¡“åŸç†å’Œæ‡‰ç”¨
- å®Œæ•´æ€§: æ¶µè“‹ä¸»è¦æ–¹é¢

ç¼ºå£è­˜åˆ¥ï¼š
- éœ€è¦æ›´å¤šæŠ€è¡“ç´°ç¯€
- ç¼ºå°‘å¯¦éš›æ‡‰ç”¨æ¡ˆä¾‹
- éœ€è¦è£œå……å¸‚å ´åˆ†æ
"""

        first_think = think_tool.invoke({"reflection": first_reflection})
        print(f"âœ… ç¬¬ä¸€è¼ªåæ€å®Œæˆ")

        # åŸºæ–¼åæ€çš„æ·±åº¦åˆ†æ
        depth_prompt = f"""
åŸºæ–¼ä»¥ä¸‹åˆå§‹åˆ†æå’Œåæ€ï¼Œæä¾›æ›´æ·±å…¥çš„åˆ†æï¼š

åˆå§‹åˆ†æï¼š
{initial_response.content[:400]}...

åæ€è¦é»ï¼š
{first_reflection}

è«‹æä¾›ï¼š
1. æ›´è©³ç´°çš„æŠ€è¡“åŸç†èªªæ˜
2. å…·é«”çš„æ‡‰ç”¨æ¡ˆä¾‹
3. å¸‚å ´è¦æ¨¡å’ŒæŠ•è³‡æƒ…æ³
4. æœªä¾†ç™¼å±•é æ¸¬
"""

        depth_response = await model.ainvoke([HumanMessage(content=depth_prompt)])
        print(f"âœ… æ·±åº¦åˆ†æ: {len(depth_response.content)} å­—ç¬¦")

        # ç¬¬äºŒè¼ªåæ€
        second_reflection = f"""
æ·±åº¦åˆ†æè©•ä¼°ï¼š
- åˆ†æé•·åº¦: {len(depth_response.content)} å­—ç¬¦
- å…§å®¹æ·±åº¦: åŒ…å«æŠ€è¡“ç´°ç¯€å’Œæ¡ˆä¾‹
- å®Œæ•´æ€§: æ¶µè“‹å¸‚å ´å’Œé æ¸¬

å“è³ªæå‡ï¼š
- æŠ€è¡“ç´°ç¯€æ›´è±å¯Œ
- åŒ…å«å¯¦éš›æ¡ˆä¾‹
- æä¾›å¸‚å ´åˆ†æ
- åŒ…å«æœªä¾†é æ¸¬

æœ€çµ‚è©•ä¼°ï¼š
- åˆ†ææ·±åº¦: å„ªç§€
- å¯¦ç”¨åƒ¹å€¼: é«˜
- å°ˆæ¥­ç¨‹åº¦: é«˜
"""

        second_think = think_tool.invoke({"reflection": second_reflection})
        print(f"âœ… ç¬¬äºŒè¼ªåæ€å®Œæˆ")

        example_3_result = {
            "initial_length": len(initial_response.content),
            "depth_length": len(depth_response.content),
            "total_length": len(initial_response.content) + len(depth_response.content),
            "reflection_rounds": 2
        }

    except Exception as e:
        print(f"âŒ ç¯„ä¾‹ 3 å¤±æ•—: {e}")
        example_3_result = None

    # è¼¸å‡ºç¸½çµ
    print("\nğŸ“Š ç¯„ä¾‹é‹è¡Œç¸½çµ")
    print("=" * 60)

    if example_1_result:
        print("âœ… ç¯„ä¾‹ 1 (åŸºæœ¬æ•´åˆ): æˆåŠŸ")
        print(f"   - åŸå§‹å›æ‡‰: {example_1_result['original_length']} å­—ç¬¦")
        print(f"   - æ”¹é€²å»ºè­°: {example_1_result['improvement_length']} å­—ç¬¦")
        print(f"   - ç¸½é•·åº¦: {example_1_result['total_length']} å­—ç¬¦")

    if example_2_result:
        print("âœ… ç¯„ä¾‹ 2 (çµæ§‹åŒ–è¼¸å‡º): æˆåŠŸ")
        print(f"   - æ¾„æ¸…å•é¡Œ: {example_2_result['question_length']} å­—ç¬¦")
        print(f"   - é¸é …æ•¸é‡: {example_2_result['options_count']}")
        print(f"   - åæ€é•·åº¦: {example_2_result['reflection_length']} å­—ç¬¦")

    if example_3_result:
        print("âœ… ç¯„ä¾‹ 3 (å¤šè¼ªåæ€): æˆåŠŸ")
        print(f"   - åˆå§‹åˆ†æ: {example_3_result['initial_length']} å­—ç¬¦")
        print(f"   - æ·±åº¦åˆ†æ: {example_3_result['depth_length']} å­—ç¬¦")
        print(f"   - ç¸½é•·åº¦: {example_3_result['total_length']} å­—ç¬¦")
        print(f"   - åæ€è¼ªæ•¸: {example_3_result['reflection_rounds']}")

    print("\nğŸ’¡ ä½¿ç”¨å»ºè­°:")
    print("1. model.invoke æä¾›éˆæ´»çš„æ¨¡å‹èª¿ç”¨èƒ½åŠ›")
    print("2. think_tool æä¾›æˆ°ç•¥åæ€å’Œæ±ºç­–æ”¯æŒ")
    print("3. çµæ§‹åŒ–è¼¸å‡ºç¢ºä¿æ•¸æ“šæ ¼å¼ä¸€è‡´æ€§")
    print("4. å¤šè¼ªåæ€å¯ä»¥é¡¯è‘—æå‡è¼¸å‡ºå“è³ª")
    print("5. çµ„åˆä½¿ç”¨å¯ä»¥å¯¦ç¾å®Œæ•´çš„å·¥ä½œæµç¨‹")

    print("\nğŸ‰ ç¯„ä¾‹æ¼”ç¤ºå®Œæˆï¼")


if __name__ == "__main__":
    asyncio.run(demonstrate_model_think_integration())
