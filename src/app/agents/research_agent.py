"""
ç ”ç©¶ä»£ç†
è² è²¬åˆ†æç”¨æˆ¶è¨Šæ¯ä¸¦åœ¨éœ€è¦æ™‚è©¢å•æ¾„æ¸…å•é¡Œ
"""
from typing import Dict, Any
from datetime import datetime
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.runnables import RunnableConfig

from ..config import Configuration, get_api_key_for_model, get_llm_with_structured_output
from ..state.research_state import ClarifyWithUser, ReportPlan
from ..state.agent_state import AgentState
from ..tools.simple_tools import tavily_search_tool


# ç ”ç©¶è¨ˆåŠƒç”ŸæˆæŒ‡ä»¤
RESEARCH_PLAN_INSTRUCTIONS = """
ä½ æ˜¯ä¸€å€‹ç ”ç©¶ç¶“ç†ï¼Œç®¡ç†è‘—ä¸€å€‹ç ”ç©¶ä»£ç†åœ˜éšŠã€‚ä»Šå¤©çš„æ—¥æœŸæ˜¯ {date}ã€‚
çµ¦å®šä¸€å€‹ç ”ç©¶æŸ¥è©¢ï¼Œä½ çš„å·¥ä½œæ˜¯ç”¢ç”Ÿå ±å‘Šçš„åˆå§‹å¤§ç¶±ï¼ˆç« ç¯€æ¨™é¡Œå’Œé—œéµå•é¡Œï¼‰ï¼Œ
ä»¥åŠä¸€äº›èƒŒæ™¯è„ˆçµ¡ã€‚æ¯å€‹ç« ç¯€å°‡åˆ†é…çµ¦åœ˜éšŠä¸­çš„ä¸åŒç ”ç©¶å“¡ï¼Œä»–å€‘å°‡å°è©²ç« ç¯€é€²è¡Œç ”ç©¶ã€‚

ä½ å°‡ç²å¾—ï¼š
- åˆå§‹ç ”ç©¶æŸ¥è©¢

ä½ çš„ä»»å‹™æ˜¯ï¼š
1. é€šéåŸ·è¡Œç¶²è·¯æœå°‹æˆ–çˆ¬å–ç¶²ç«™ï¼Œç”¢ç”Ÿ1-2æ®µåˆå§‹èƒŒæ™¯è„ˆçµ¡ï¼ˆå¦‚éœ€è¦ï¼‰
2. ç”¢ç”Ÿå ±å‘Šå¤§ç¶±ï¼ŒåŒ…æ‹¬ç« ç¯€æ¨™é¡Œåˆ—è¡¨å’Œæ¯å€‹ç« ç¯€è¦è§£æ±ºçš„é—œéµå•é¡Œ
3. æä¾›å ±å‘Šæ¨™é¡Œï¼Œå°‡ç”¨ä½œä¸»è¦æ¨™é¡Œ

æŒ‡å°åŸå‰‡ï¼š
- æ¯å€‹ç« ç¯€æ‡‰æ¶µè“‹ä¸€å€‹ç¨ç«‹æ–¼å…¶ä»–ç« ç¯€çš„å–®ä¸€ä¸»é¡Œ/å•é¡Œ
- æ¯å€‹ç« ç¯€çš„é—œéµå•é¡Œæ‡‰åŒ…æ‹¬åç¨±å’ŒåŸŸå/ç¶²ç«™ï¼ˆå¦‚æœå¯ç”¨ä¸”é©ç”¨ï¼‰ï¼Œå¦‚æœèˆ‡å…¬å¸ã€ç”¢å“æˆ–é¡ä¼¼äº‹ç‰©ç›¸é—œ
- èƒŒæ™¯è„ˆçµ¡ä¸æ‡‰è¶…é2æ®µ
- èƒŒæ™¯è„ˆçµ¡æ‡‰éå¸¸å…·é«”åœ°é‡å°æŸ¥è©¢ï¼Œä¸¦åŒ…æ‹¬èˆ‡å ±å‘Šæ‰€æœ‰ç« ç¯€çš„ç ”ç©¶å“¡ç›¸é—œçš„ä»»ä½•è³‡è¨Š
- èƒŒæ™¯è„ˆçµ¡æ‡‰åƒ…ä¾†è‡ªç¶²è·¯æœå°‹æˆ–çˆ¬å–çµæœï¼Œè€Œä¸æ˜¯å…ˆé©—çŸ¥è­˜ï¼ˆå³åªæœ‰åœ¨èª¿ç”¨å·¥å…·æ™‚æ‰æ‡‰åŒ…å«ï¼‰
- ä¾‹å¦‚ï¼Œå¦‚æœæŸ¥è©¢æ˜¯é—œæ–¼ä¸€å®¶å…¬å¸ï¼ŒèƒŒæ™¯è„ˆçµ¡æ‡‰åŒ…æ‹¬é—œæ–¼è©²å…¬å¸åšä»€éº¼çš„ä¸€äº›åŸºæœ¬è³‡è¨Š
- ä¸è¦é€²è¡Œè¶…é2æ¬¡å·¥å…·èª¿ç”¨

åªè¼¸å‡ºJSONã€‚éµå¾ªä¸‹é¢çš„JSONæ¶æ§‹ã€‚ä¸è¦è¼¸å‡ºä»»ä½•å…¶ä»–å…§å®¹ã€‚æˆ‘å°‡ä½¿ç”¨Pydanticè§£ææ­¤å…§å®¹ï¼Œæ‰€ä»¥åªè¼¸å‡ºæœ‰æ•ˆçš„JSONï¼š
{json_schema}
"""


async def clarify_with_user(state: AgentState, config: RunnableConfig) -> ClarifyWithUser:
    """åˆ†æç”¨æˆ¶è¨Šæ¯ä¸¦æä¾›æ¾„æ¸…å•é¡Œå’Œç ”ç©¶æ–¹å‘é¸é …ã€‚

    æ­¤å‡½æ•¸çµ±ä¸€åŸ·è¡Œæ¾„æ¸…åˆ†æï¼Œå§‹çµ‚æä¾›æ¾„æ¸…å•é¡Œå’Œ3å€‹ç ”ç©¶æ–¹å‘é¸é …ã€‚
    ä¸å†æ ¹æ“š need_clarification ä¾†æ±ºå®šæ˜¯å¦æ¾„æ¸…ã€‚

    Args:
        state: åŒ…å«ç”¨æˆ¶è¨Šæ¯çš„ç•¶å‰ä»£ç†ç‹€æ…‹
        config: åŒ…å«æ¨¡å‹è¨­ç½®å’Œåå¥½çš„é‹è¡Œæ™‚é…ç½®

    Returns:
        åŒ…å«æ¾„æ¸…åˆ†æçµæœçš„å­—å…¸
    """
    # æ­¥é©Ÿ 1ï¼šç²å–é…ç½®
    configurable = Configuration.from_runnable_config(config)

    # æ­¥é©Ÿ 2ï¼šç‚ºçµæ§‹åŒ–æ¾„æ¸…åˆ†ææº–å‚™æ¨¡å‹
    messages = state["messages"]  # æ”¯æ´å­—å…¸å’Œå°è±¡è¨ªå•
    model_config = {
        "model": configurable.research_model,
        "max_tokens": configurable.research_model_max_tokens,
        "api_key": get_api_key_for_model(configurable.research_model, config),
        "tags": ["langsmith:nostream"]
    }

    # é…ç½®å…·æœ‰çµæ§‹åŒ–è¼¸å‡ºå’Œé‡è©¦é‚è¼¯çš„æ¨¡å‹
    clarification_model = get_llm_with_structured_output(
        configurable.research_model,
        ClarifyWithUser  # å®šç¾©å›å‚³å…§å®¹æ ¼å¼
    )

    # æ­¥é©Ÿ 3ï¼šç”Ÿæˆæ¾„æ¸…å•é¡Œå’Œé¸é …
    # å»ºç«‹æ¾„æ¸…æç¤º
    clarify_prompt = f"""
    ä½ æ˜¯ä¸€å€‹ç ”ç©¶åˆ†æå¸«ï¼Œéœ€è¦ç‚ºç”¨æˆ¶çš„ç ”ç©¶è«‹æ±‚æä¾›æ¾„æ¸…å•é¡Œå’Œç ”ç©¶æ–¹å‘é¸é …ã€‚

    ç”¨æˆ¶çš„è«‹æ±‚ï¼š
    {get_buffer_string(messages)}

    è«‹ç‚ºé€™å€‹è«‹æ±‚æä¾›ä¸€å€‹æ¾„æ¸…å•é¡Œå’Œ3å€‹å…·é«”çš„ç ”ç©¶æ–¹å‘é¸é …ï¼Œå¹«åŠ©ç”¨æˆ¶æ›´æ˜ç¢ºåœ°å®šç¾©ç ”ç©¶ç¯„åœã€‚

    å§‹çµ‚æä¾›æ¾„æ¸…å•é¡Œå’Œ3å€‹ç ”ç©¶æ–¹å‘é¸é …ï¼Œç„¡è«–è«‹æ±‚æ˜¯å¦æ˜ç¢ºã€‚
    """

    response = await clarification_model.ainvoke([HumanMessage(content=clarify_prompt)])

    # æ­¥é©Ÿ 4ï¼šè¿”å›æ¾„æ¸…åˆ†æçµæœï¼ˆçµ±ä¸€è¨­ç½®ç‚ºéœ€è¦æ¾„æ¸…ï¼‰
    return {
        "question": response.question,
        "options": response.options,
        "verification": response.verification
    }


async def write_research_brief(state: AgentState, config: RunnableConfig) -> Dict[str, Any]:
    """ç”Ÿæˆç ”ç©¶ç°¡å ±å’Œå ±å‘Šè¨ˆåŠƒã€‚

    æ­¤å‡½æ•¸æ ¹æ“šç”¨æˆ¶æŸ¥è©¢ç”Ÿæˆè©³ç´°çš„ç ”ç©¶è¨ˆåŠƒï¼ŒåŒ…æ‹¬èƒŒæ™¯è„ˆçµ¡ã€å ±å‘Šå¤§ç¶±å’Œæ¨™é¡Œã€‚
    æœƒä½¿ç”¨ Tavily æœå°‹å·¥å…·ç²å–ç›¸é—œçš„èƒŒæ™¯è³‡è¨Šã€‚

    Args:
        state: åŒ…å«ç”¨æˆ¶è¨Šæ¯çš„ç•¶å‰ä»£ç†ç‹€æ…‹
        config: åŒ…å«æ¨¡å‹è¨­ç½®å’Œåå¥½çš„é‹è¡Œæ™‚é…ç½®

    Returns:
        åŒ…å«ç ”ç©¶è¨ˆåŠƒçµæœçš„å­—å…¸
    """
    # æ­¥é©Ÿ 1ï¼šç²å–é…ç½®å’Œç”¨æˆ¶è¨Šæ¯
    configurable = Configuration.from_runnable_config(config)
    messages = state["messages"]  # æ”¯æ´å­—å…¸å’Œå°è±¡è¨ªå•
    user_query = get_buffer_string(messages)

    # æ­¥é©Ÿ 2ï¼šä½¿ç”¨ Tavily å·¥å…·é€²è¡ŒèƒŒæ™¯æœå°‹
    search_results = []
    try:
        # ä½¿ç”¨ @tool è£é£¾å™¨çš„ç°¡åŒ–æ–¹å¼
        search_results = tavily_search_tool.invoke({
            "query": user_query,
            "max_results": 3,
            "search_depth": "basic"
        })
        print(f"ğŸ” Tavily æœå°‹å®Œæˆï¼Œæ‰¾åˆ° {len(search_results)} å€‹çµæœ")
    except Exception as e:
        print(f"âš ï¸ Tavily æœå°‹å¤±æ•—: {e}")
        search_results = []

    # æ­¥é©Ÿ 3ï¼šæº–å‚™æ¨¡å‹é…ç½®
    model_config = {
        "model": configurable.plan_model,
        "max_tokens": configurable.plan_model_max_tokens,
        "api_key": get_api_key_for_model(configurable.plan_model, config),
        "tags": ["langsmith:nostream"]
    }

    # æ­¥é©Ÿ 4ï¼šé…ç½®å…·æœ‰çµæ§‹åŒ–è¼¸å‡ºçš„æ¨¡å‹
    plan_model = get_llm_with_structured_output(
        configurable.plan_model,
        ReportPlan
    )

    # æ­¥é©Ÿ 5ï¼šæº–å‚™åŒ…å«æœå°‹çµæœçš„æç¤º
    prompt_content = RESEARCH_PLAN_INSTRUCTIONS.format(
        date=datetime.now().strftime("%Y-%m-%d"),
        json_schema=ReportPlan.model_json_schema()
    )

    # æ·»åŠ æœå°‹çµæœåˆ°æç¤ºä¸­
    search_context = ""
    if search_results and not any("error" in result for result in search_results):
        search_context = "\n\næœå°‹åˆ°çš„èƒŒæ™¯è³‡è¨Šï¼š\n"
        for i, result in enumerate(search_results, 1):
            search_context += f"{i}. {result.get('title', 'ç„¡æ¨™é¡Œ')}\n"
            search_context += f"   å…§å®¹: {result.get('content', 'ç„¡å…§å®¹')[:200]}...\n"
            search_context += f"   ä¾†æº: {result.get('url', 'ç„¡URL')}\n\n"

    full_prompt = f"{prompt_content}\n\nç ”ç©¶æŸ¥è©¢ï¼š\n{user_query}{search_context}"

    # æ­¥é©Ÿ 6ï¼šç”Ÿæˆç ”ç©¶è¨ˆåŠƒ
    response = await plan_model.ainvoke([HumanMessage(content=full_prompt)])

    # æ­¥é©Ÿ 7ï¼šè¿”å›ç ”ç©¶è¨ˆåŠƒçµæœ
    return {
        "background_context": response.background_context,
        "report_outline": [section.model_dump() for section in response.report_outline],
        "report_title": response.report_title,
        "search_results": search_results  # åŒ…å«æœå°‹çµæœä¾›å¾ŒçºŒä½¿ç”¨
    }


# è¼”åŠ©å‡½æ•¸
def get_buffer_string(messages) -> str:
    """å°‡è¨Šæ¯åˆ—è¡¨è½‰æ›ç‚ºå­—ä¸²"""
    return "\n".join([msg.content for msg in messages if hasattr(msg, 'content')])


def get_today_str() -> str:
    """ç²å–ä»Šå¤©çš„æ—¥æœŸå­—ä¸²"""
    return datetime.now().strftime("%Y-%m-%d")


# æ¸¬è©¦å‡½æ•¸
async def test_research_agent():
    """æ¸¬è©¦ç ”ç©¶ä»£ç†åŠŸèƒ½"""
    from langchain_core.messages import HumanMessage

    # æ¨¡æ“¬ç‹€æ…‹
    test_state = {
        "messages": [HumanMessage(content="æˆ‘æƒ³äº†è§£äººå·¥æ™ºæ…§åœ¨é†«ç™‚é ˜åŸŸçš„æœ€æ–°æ‡‰ç”¨")]
    }

    # æ¨¡æ“¬é…ç½®
    test_config = {
        "configurable": {
            "allow_clarification": True,
            "research_model": "gemini-2.5-pro",
            "plan_model": "gemini-2.5-pro",
            "research_model_max_tokens": 10000,
            "plan_model_max_tokens": 8192
        }
    }

    print("ğŸ§ª æ¸¬è©¦ç ”ç©¶ä»£ç†")
    print("=" * 40)

    # æ¸¬è©¦æ¾„æ¸…åŠŸèƒ½
    print("1. æ¸¬è©¦æ¾„æ¸…åŠŸèƒ½...")
    try:
        clarify_result = await clarify_with_user(test_state, test_config)
        print(f"   éœ€è¦æ¾„æ¸…: {clarify_result['need_clarification']}")
        if clarify_result['need_clarification']:
            print(f"   æ¾„æ¸…å•é¡Œ: {clarify_result['question']}")
            print(f"   é¸é …: {clarify_result['options']}")
        else:
            print(f"   ç¢ºèªè¨Šæ¯: {clarify_result['verification']}")
    except Exception as e:
        print(f"   æ¾„æ¸…æ¸¬è©¦å¤±æ•—: {e}")

    print("\n2. æ¸¬è©¦ç ”ç©¶è¨ˆåŠƒç”Ÿæˆ...")
    try:
        plan_result = await write_research_brief(test_state, test_config)
        print(f"   å ±å‘Šæ¨™é¡Œ: {plan_result['report_title']}")
        print(f"   èƒŒæ™¯è„ˆçµ¡: {plan_result['background_context'][:100]}...")
        print(f"   ç« ç¯€æ•¸é‡: {len(plan_result['report_outline'])}")
        print(f"   æœå°‹çµæœæ•¸é‡: {len(plan_result.get('search_results', []))}")
    except Exception as e:
        print(f"   è¨ˆåŠƒç”Ÿæˆæ¸¬è©¦å¤±æ•—: {e}")


if __name__ == "__main__":
    import asyncio
    asyncio.run(test_research_agent())
