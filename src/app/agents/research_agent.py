"""æ·±åº¦ç ”ç©¶ä»£ç†çš„ä¸»è¦ LangGraph å¯¦ä½œã€‚"""

import asyncio
from typing import Literal
import time
from datetime import datetime

from langchain.chat_models import init_chat_model
from langchain_core.messages import (
    AIMessage,
    HumanMessage,
    SystemMessage,
    ToolMessage,
    filter_messages,
    get_buffer_string,
)
from langchain_core.runnables import RunnableConfig
from langgraph.graph import END, START, StateGraph
from langgraph.types import Command

from ..config import (
    Configuration,
)
from ..prompt import (
    clarify_with_user_instructions,
    compress_research_simple_human_message,
    compress_research_system_prompt,
    final_report_generation_prompt,
    lead_researcher_prompt,
    research_system_prompt,
    transform_messages_into_research_topic_prompt,
)
from ..state.research_state import (
    AgentInputState,
    AgentState,
    ClarifyWithUser,
    ConductResearch,
    ResearchComplete,
    ResearcherOutputState,
    ResearcherState,
    ResearchQuestion,
    SupervisorState,
)
from ..utils.utils import (
    anthropic_websearch_called,
    get_all_tools,
    get_api_key_for_model,
    get_model_token_limit,
    get_notes_from_tool_calls,
    get_today_str,
    is_token_limit_exceeded,
    openai_websearch_called,
    remove_up_to_last_ai_message,
    think_tool,
)

# åˆå§‹åŒ–ä¸€å€‹å¯åœ¨æ•´å€‹ä»£ç†ä¸­ä½¿ç”¨çš„å¯é…ç½®æ¨¡å‹
configurable_model = init_chat_model(
    configurable_fields=("model", "max_tokens"),
)


def print_progress(message: str, level: int = 0):
    """æ‰“å°é€²åº¦è¨Šæ¯"""
    # timestamp = datetime.now().strftime("%H:%M:%S")
    indent = "  " * level
    formatted_message = f" {indent}{message}"
    print(formatted_message)

    # å¦‚æœå­˜åœ¨å…¨åŸŸé€²åº¦å›èª¿å‡½æ•¸ï¼Œå‰‡èª¿ç”¨å®ƒ
    if hasattr(print_progress, 'progress_callback') and print_progress.progress_callback:
        try:
            print_progress.progress_callback(formatted_message)
        except Exception:
            pass  # å¿½ç•¥å›èª¿éŒ¯èª¤ï¼Œé¿å…å½±éŸ¿ä¸»è¦æµç¨‹


def _append_timing(state: dict, label: str, start_time: float) -> list[str]:
    """å°‡å–®æ­¥è€—æ™‚ï¼ˆç§’ï¼‰é™„åŠ åˆ°ç‹€æ…‹ä¸­çš„ timings ä¸¦å›å‚³æœ€æ–°åˆ—è¡¨ã€‚"""
    elapsed = time.perf_counter() - start_time
    timings = state.get("timings", []) or []

    # ç‚º supervisor ç›¸é—œæ­¥é©Ÿæ·»åŠ è¿­ä»£æ¬¡æ•¸
    if label in ["supervisor", "supervisor_tools"]:
        iteration = state.get("research_iterations", 0)
        unique_label = f"{label}#{iteration}"
    else:
        unique_label = label

    timings = [*timings, f"{unique_label}: {elapsed:.2f}s"]
    return timings


async def clarify_with_user(state: AgentState, config: RunnableConfig) -> Command[Literal["write_research_brief", "__end__"]]:
    """åˆ†æä½¿ç”¨è€…è¨Šæ¯ï¼Œå¦‚æœç ”ç©¶ç¯„åœä¸æ¸…æ¥šå‰‡æå‡ºé‡æ¸…å•é¡Œã€‚

    æ­¤å‡½æ•¸æ±ºå®šä½¿ç”¨è€…çš„è«‹æ±‚åœ¨é€²è¡Œç ”ç©¶ä¹‹å‰æ˜¯å¦éœ€è¦é‡æ¸…ã€‚å¦‚æœé‡æ¸…è¢«ç¦ç”¨æˆ–ä¸éœ€è¦ï¼Œ
    å‰‡ç›´æ¥é€²è¡Œç ”ç©¶ã€‚

    Args:
        state: åŒ…å«ä½¿ç”¨è€…è¨Šæ¯çš„ç•¶å‰ä»£ç†ç‹€æ…‹
        config: åŒ…å«æ¨¡å‹è¨­å®šå’Œåå¥½çš„é‹è¡Œæ™‚é…ç½®

    Returns:
        ä»¥é‡æ¸…å•é¡ŒçµæŸæˆ–é€²è¡Œç ”ç©¶ç°¡å ±çš„å‘½ä»¤
    """
    # æ¨™æº–åŒ–éšæ®µäº‹ä»¶ï¼šä½¿ç”¨è€…é‡æ¸…éšæ®µ
    print_progress("STAGE::clarify_with_user::enter")
    # print_progress("ğŸ” æ­£åœ¨åˆ†æç ”ç©¶éœ€æ±‚...")

    # æ­¥é©Ÿ 1ï¼šæª¢æŸ¥é…ç½®ä¸­æ˜¯å¦å•Ÿç”¨é‡æ¸…
    _start_t = time.perf_counter()
    configurable = Configuration.from_runnable_config(config)
    # print_progress("âœ“ å·²è¼‰å…¥é…ç½®è¨­å®š", 1)

    if not configurable.allow_clarification:
        # è·³éé‡æ¸…æ­¥é©Ÿï¼Œç›´æ¥é€²è¡Œç ”ç©¶
        # print_progress("âœ“ é‡æ¸…åŠŸèƒ½å·²ç¦ç”¨ï¼Œè·³éé‡æ¸…æ­¥é©Ÿ", 1)
        return Command(goto="write_research_brief")

    # æ­¥é©Ÿ 2ï¼šç‚ºçµæ§‹åŒ–é‡æ¸…åˆ†ææº–å‚™æ¨¡å‹
    # print_progress("âœ“ æ­£åœ¨æº–å‚™é‡æ¸…åˆ†ææ¨¡å‹...", 1)
    messages = state["messages"]
    model_config = {
        "model": configurable.research_model,
        "max_tokens": configurable.research_model_max_tokens,
        "tags": ["langsmith:nostream"]
    }

    # é…ç½®å…·æœ‰çµæ§‹åŒ–è¼¸å‡ºå’Œé‡è©¦é‚è¼¯çš„æ¨¡å‹
    clarification_model = (
        configurable_model
        .with_structured_output(ClarifyWithUser)
        .with_retry(stop_after_attempt=configurable.max_structured_output_retries)
        .with_config(model_config)
    )

    # æ­¥é©Ÿ 3ï¼šåˆ†ææ˜¯å¦éœ€è¦é‡æ¸…
    # print_progress("âœ“ æ­£åœ¨åˆ†æä½¿ç”¨è€…éœ€æ±‚æ˜¯å¦æ˜ç¢º...", 1)
    prompt_content = clarify_with_user_instructions.format(
        messages=get_buffer_string(messages),
        date=get_today_str()
    )
    response = await clarification_model.ainvoke([HumanMessage(content=prompt_content)])

    # æ­¥é©Ÿ 4ï¼šæ ¹æ“šé‡æ¸…åˆ†æé€²è¡Œè·¯ç”±
    if response.need_clarification:
        # ä»¥é‡æ¸…å•é¡ŒçµæŸçµ¦ä½¿ç”¨è€…
        # print_progress("âœ“ éœ€è¦é‡æ¸…å•é¡Œï¼Œç­‰å¾…ä½¿ç”¨è€…å›æ‡‰", 1)
        return Command(
            goto=END,
            update={
                "messages": [AIMessage(content=response.question)],
                "timings": _append_timing(state, "clarify_with_user", _start_t)
            }
        )
    else:
        # ä»¥é©—è­‰è¨Šæ¯é€²è¡Œç ”ç©¶
        # print_progress("âœ“ éœ€æ±‚æ˜ç¢ºï¼Œæº–å‚™é€²è¡Œç ”ç©¶", 1)
        return Command(
            goto="write_research_brief",
            update={
                "messages": [AIMessage(content=response.verification)],
                "timings": _append_timing(state, "clarify_with_user", _start_t)
            }
        )


async def write_research_brief(state: AgentState, config: RunnableConfig) -> Command[Literal["research_supervisor"]]:
    """å°‡ä½¿ç”¨è€…è¨Šæ¯è½‰æ›ç‚ºçµæ§‹åŒ–ç ”ç©¶ç°¡å ±ä¸¦åˆå§‹åŒ–ç›£ç£è€…ã€‚

    æ­¤å‡½æ•¸åˆ†æä½¿ç”¨è€…çš„è¨Šæ¯ä¸¦ç”Ÿæˆä¸€å€‹å°ˆæ³¨çš„ç ”ç©¶ç°¡å ±ï¼Œå°‡æŒ‡å°ç ”ç©¶ç›£ç£è€…ã€‚
    å®ƒé‚„ä½¿ç”¨é©ç•¶çš„æç¤ºå’ŒæŒ‡ç¤ºè¨­ç½®åˆå§‹ç›£ç£è€…ä¸Šä¸‹æ–‡ã€‚

    Args:
        state: åŒ…å«ä½¿ç”¨è€…è¨Šæ¯çš„ç•¶å‰ä»£ç†ç‹€æ…‹
        config: åŒ…å«æ¨¡å‹è¨­å®šçš„é‹è¡Œæ™‚é…ç½®

    Returns:
        é€²è¡Œç ”ç©¶ç›£ç£è€…çš„å‘½ä»¤ï¼Œå¸¶æœ‰åˆå§‹åŒ–çš„ä¸Šä¸‹æ–‡
    """
    # æ¨™æº–åŒ–éšæ®µäº‹ä»¶ï¼šç ”ç©¶è¦åŠƒéšæ®µ
    print_progress("STAGE::write_research_brief::enter")
    # print_progress("ğŸ“‹ æ­£åœ¨åˆ¶å®šç ”ç©¶è¨ˆç•«...")

    # æ­¥é©Ÿ 1ï¼šç‚ºçµæ§‹åŒ–è¼¸å‡ºè¨­ç½®ç ”ç©¶æ¨¡å‹
    _start_t = time.perf_counter()
    configurable = Configuration.from_runnable_config(config)
    # print_progress("âœ“ å·²è¼‰å…¥ç ”ç©¶é…ç½®", 1)

    research_model_config = {
        "model": configurable.research_model,
        "max_tokens": configurable.research_model_max_tokens,
        "tags": ["langsmith:nostream"]
    }

    # ç‚ºçµæ§‹åŒ–ç ”ç©¶å•é¡Œç”Ÿæˆé…ç½®æ¨¡å‹
    # print_progress("âœ“ æ­£åœ¨é…ç½®ç ”ç©¶å•é¡Œåˆ†ææ¨¡å‹...", 1)
    research_model = (
        configurable_model
        .with_structured_output(ResearchQuestion)
        .with_retry(stop_after_attempt=configurable.max_structured_output_retries)
        .with_config(research_model_config)
    )

    # æ­¥é©Ÿ 2ï¼šå¾ä½¿ç”¨è€…è¨Šæ¯ç”Ÿæˆçµæ§‹åŒ–ç ”ç©¶ç°¡å ±
    # print_progress("âœ“ æ­£åœ¨åˆ†æä½¿ç”¨è€…è¨Šæ¯ä¸¦ç”Ÿæˆç ”ç©¶ç°¡å ±...", 1)
    prompt_content = transform_messages_into_research_topic_prompt.format(
        messages=get_buffer_string(state.get("messages", [])),
        date=get_today_str()
    )
    response = await research_model.ainvoke([HumanMessage(content=prompt_content)])
    # print_progress("âœ“ ç ”ç©¶æ–¹å‘èˆ‡æ¶æ§‹å·²ç”Ÿæˆ", 1)
    # å°‡ç ”ç©¶ç°¡å ±å…§å®¹å³æ™‚æ¨é€çµ¦å‰ç«¯
    # print_progress(f"BRIEF::{response.research_brief}")

    # æ­¥é©Ÿ 3ï¼šä½¿ç”¨ç ”ç©¶ç°¡å ±å’ŒæŒ‡ç¤ºåˆå§‹åŒ–ç›£ç£è€…
    # print_progress("âœ“ æ­£åœ¨åˆå§‹åŒ–ç ”ç©¶ç›£ç£è€…...", 1)
    supervisor_system_prompt = lead_researcher_prompt.format(
        date=get_today_str(),
        max_concurrent_research_units=configurable.max_concurrent_research_units,
        max_researcher_iterations=configurable.max_researcher_iterations
    )
    # print_progress("âœ“ ç›£ç£è€…å·²æº–å‚™å°±ç·’", 1)

    return Command(
        goto="research_supervisor",
        update={
            "research_brief": response.research_brief,
            "supervisor_messages": {
                "type": "override",
                "value": [
                    SystemMessage(content=supervisor_system_prompt),
                    HumanMessage(content=response.research_brief)
                ]
            },
            "timings": _append_timing(state, "write_research_brief", _start_t)
        }
    )


async def supervisor(state: SupervisorState, config: RunnableConfig) -> Command[Literal["supervisor_tools"]]:
    """é ˜å°ç ”ç©¶ç›£ç£è€…ï¼Œè¦åŠƒç ”ç©¶ç­–ç•¥ä¸¦å§”æ´¾çµ¦ç ”ç©¶è€…ã€‚

    ç›£ç£è€…åˆ†æç ”ç©¶ç°¡å ±ä¸¦æ±ºå®šå¦‚ä½•å°‡ç ”ç©¶åˆ†è§£ç‚ºå¯ç®¡ç†çš„ä»»å‹™ã€‚å®ƒå¯ä»¥ä½¿ç”¨ think_tool
    é€²è¡Œæˆ°ç•¥è¦åŠƒï¼Œä½¿ç”¨ ConductResearch å°‡ä»»å‹™å§”æ´¾çµ¦å­ç ”ç©¶è€…ï¼Œæˆ–åœ¨æ»¿æ„ç™¼ç¾æ™‚ä½¿ç”¨
    ResearchCompleteã€‚

    Args:
        state: åŒ…å«è¨Šæ¯å’Œç ”ç©¶ä¸Šä¸‹æ–‡çš„ç•¶å‰ç›£ç£è€…ç‹€æ…‹
        config: åŒ…å«æ¨¡å‹è¨­å®šçš„é‹è¡Œæ™‚é…ç½®

    Returns:
        é€²è¡Œ supervisor_tools é€²è¡Œå·¥å…·åŸ·è¡Œçš„å‘½ä»¤
    """
    # æ¨™æº–åŒ–éšæ®µäº‹ä»¶ï¼šç ”ç©¶åŸ·è¡Œï¼ˆç›£ç£è€…ï¼‰éšæ®µï¼Œå«è¿­ä»£æ¬¡æ•¸
    current_iteration = state.get("research_iterations", 0) + 1
    print_progress(
        f"STAGE::research_supervisor::enter::iteration={current_iteration}")
    # print_progress("ğŸ¯ æ­£åœ¨è¦åŠƒç ”ç©¶ç­–ç•¥...")

    # æ­¥é©Ÿ 1ï¼šä½¿ç”¨å¯ç”¨å·¥å…·é…ç½®ç›£ç£è€…æ¨¡å‹
    _start_t = time.perf_counter()
    configurable = Configuration.from_runnable_config(config)
    # print_progress("âœ“ å·²è¼‰å…¥ç›£ç£è€…é…ç½®", 1)

    research_model_config = {
        "model": configurable.research_model,
        "max_tokens": configurable.research_model_max_tokens,
        "tags": ["langsmith:nostream"]
    }

    # å¯ç”¨å·¥å…·ï¼šç ”ç©¶å§”æ´¾ã€å®Œæˆä¿¡è™Ÿå’Œæˆ°ç•¥æ€è€ƒ
    # print_progress("âœ“ æ­£åœ¨é…ç½®å¯ç”¨å·¥å…·ï¼šç ”ç©¶å§”æ´¾ã€å®Œæˆä¿¡è™Ÿã€æ·±åº¦æ€è€ƒ", 1)
    lead_researcher_tools = [ConductResearch, ResearchComplete, think_tool]

    # é…ç½®å…·æœ‰å·¥å…·ã€é‡è©¦é‚è¼¯å’Œæ¨¡å‹è¨­å®šçš„æ¨¡å‹
    # print_progress("âœ“ æ­£åœ¨ç¶å®šå·¥å…·ä¸¦è¨­ç½®é‡è©¦é‚è¼¯...", 1)
    research_model = (
        configurable_model
        .bind_tools(lead_researcher_tools)
        .with_retry(stop_after_attempt=configurable.max_structured_output_retries)
        .with_config(research_model_config)
    )

    # æ­¥é©Ÿ 2ï¼šåŸºæ–¼ç•¶å‰ä¸Šä¸‹æ–‡ç”Ÿæˆç›£ç£è€…å›æ‡‰
    # print_progress("âœ“ æ­£åœ¨åˆ†æç ”ç©¶ç°¡å ±ä¸¦åˆ¶å®šç­–ç•¥...", 1)
    supervisor_messages = state.get("supervisor_messages", [])
    response = await research_model.ainvoke(supervisor_messages)
    # print_progress("âœ“ ç ”ç©¶ç­–ç•¥å·²åˆ¶å®š", 1)

    # æ­¥é©Ÿ 3ï¼šæ›´æ–°ç‹€æ…‹ä¸¦é€²è¡Œå·¥å…·åŸ·è¡Œ
    return Command(
        goto="supervisor_tools",
        update={
            "supervisor_messages": [response],
            "research_iterations": state.get("research_iterations", 0) + 1,
            "timings": _append_timing(state, "supervisor", _start_t)
        }
    )


async def supervisor_tools(state: SupervisorState, config: RunnableConfig) -> Command[Literal["supervisor", "__end__"]]:
    """åŸ·è¡Œç›£ç£è€…èª¿ç”¨çš„å·¥å…·ï¼ŒåŒ…æ‹¬ç ”ç©¶å§”æ´¾å’Œæˆ°ç•¥æ€è€ƒã€‚

    æ­¤å‡½æ•¸è™•ç†ä¸‰ç¨®é¡å‹çš„ç›£ç£è€…å·¥å…·èª¿ç”¨ï¼š
    1. think_tool - ç¹¼çºŒå°è©±çš„æˆ°ç•¥åæ€
    2. ConductResearch - å°‡ç ”ç©¶ä»»å‹™å§”æ´¾çµ¦å­ç ”ç©¶è€…
    3. ResearchComplete - ä¿¡è™Ÿç ”ç©¶éšæ®µå®Œæˆ

    Args:
        state: åŒ…å«è¨Šæ¯å’Œè¿­ä»£è¨ˆæ•¸çš„ç•¶å‰ç›£ç£è€…ç‹€æ…‹
        config: åŒ…å«ç ”ç©¶é™åˆ¶å’Œæ¨¡å‹è¨­å®šçš„é‹è¡Œæ™‚é…ç½®

    Returns:
        ç¹¼çºŒç›£ç£å¾ªç’°æˆ–çµæŸç ”ç©¶éšæ®µçš„å‘½ä»¤
    """
    # æ­¥é©Ÿ 1ï¼šæå–ç•¶å‰ç‹€æ…‹ä¸¦æª¢æŸ¥é€€å‡ºæ¢ä»¶
    _start_t = time.perf_counter()
    configurable = Configuration.from_runnable_config(config)
    supervisor_messages = state.get("supervisor_messages", [])
    research_iterations = state.get("research_iterations", 0)
    most_recent_message = supervisor_messages[-1]

    # å®šç¾©ç ”ç©¶éšæ®µçš„é€€å‡ºæ¨™æº–
    exceeded_allowed_iterations = research_iterations > configurable.max_researcher_iterations
    no_tool_calls = not most_recent_message.tool_calls
    research_complete_tool_call = any(
        tool_call["name"] == "ResearchComplete"
        for tool_call in most_recent_message.tool_calls
    )

    # å¦‚æœæ»¿è¶³ä»»ä½•çµ‚æ­¢æ¢ä»¶å‰‡é€€å‡º
    if exceeded_allowed_iterations or no_tool_calls or research_complete_tool_call:
        return Command(
            goto=END,
            update={
                "notes": get_notes_from_tool_calls(supervisor_messages),
                "research_brief": state.get("research_brief", ""),
                "timings": _append_timing(state, "supervisor_tools", _start_t)
            }
        )

    # æ­¥é©Ÿ 2ï¼šä¸€èµ·è™•ç†æ‰€æœ‰å·¥å…·èª¿ç”¨ï¼ˆthink_tool å’Œ ConductResearchï¼‰
    all_tool_messages = []
    update_payload = {"supervisor_messages": []}

    # è™•ç† think_tool èª¿ç”¨ï¼ˆæˆ°ç•¥åæ€ï¼‰
    think_tool_calls = [
        tool_call for tool_call in most_recent_message.tool_calls
        if tool_call["name"] == "think_tool"
    ]

    for tool_call in think_tool_calls:
        reflection_content = tool_call["args"]["reflection"]
        all_tool_messages.append(ToolMessage(
            content=f"Reflection recorded: {reflection_content}",
            name="think_tool",
            tool_call_id=tool_call["id"]
        ))

    # è™•ç† ConductResearch èª¿ç”¨ï¼ˆç ”ç©¶å§”æ´¾ï¼‰
    conduct_research_calls = [
        tool_call for tool_call in most_recent_message.tool_calls
        if tool_call["name"] == "ConductResearch"
    ]

    if conduct_research_calls:
        # print_progress(f"âœ“ æ­£åœ¨å§”æ´¾ {len(conduct_research_calls)} å€‹ç ”ç©¶ä»»å‹™...", 1)
        try:
            # é™åˆ¶ä¸¦è¡Œç ”ç©¶å–®ä½ä»¥é˜²æ­¢è³‡æºè€—ç›¡
            allowed_conduct_research_calls = conduct_research_calls[
                :configurable.max_concurrent_research_units]
            overflow_conduct_research_calls = conduct_research_calls[
                configurable.max_concurrent_research_units:]

            # ä¸¦è¡ŒåŸ·è¡Œç ”ç©¶ä»»å‹™
            research_tasks = [
                researcher_subgraph.ainvoke({
                    "researcher_messages": [
                        HumanMessage(
                            content=tool_call["args"]["research_topic"])
                    ],
                    "research_topic": tool_call["args"]["research_topic"]
                }, config)
                for tool_call in allowed_conduct_research_calls
            ]

            tool_results = await asyncio.gather(*research_tasks)

            # ä½¿ç”¨ç ”ç©¶çµæœå‰µå»ºå·¥å…·è¨Šæ¯
            for observation, tool_call in zip(tool_results, allowed_conduct_research_calls):
                all_tool_messages.append(ToolMessage(
                    content=observation.get(
                        "compressed_research", "Error synthesizing research report: Maximum retries exceeded"),
                    name=tool_call["name"],
                    tool_call_id=tool_call["id"]
                ))

            # è™•ç†æº¢å‡ºçš„ç ”ç©¶èª¿ç”¨ä¸¦æä¾›éŒ¯èª¤è¨Šæ¯
            for overflow_call in overflow_conduct_research_calls:
                all_tool_messages.append(ToolMessage(
                    content=f"Error: Did not run this research as you have already exceeded the maximum number of concurrent research units. Please try again with {configurable.max_concurrent_research_units} or fewer research units.",
                    name="ConductResearch",
                    tool_call_id=overflow_call["id"]
                ))

            # èšåˆæ‰€æœ‰ç ”ç©¶çµæœçš„åŸå§‹ç­†è¨˜
            raw_notes_concat = "\n".join([
                "\n".join(observation.get("raw_notes", []))
                for observation in tool_results
            ])

            if raw_notes_concat:
                update_payload["raw_notes"] = [raw_notes_concat]

        except Exception as e:
            # è™•ç†ç ”ç©¶åŸ·è¡ŒéŒ¯èª¤
            if is_token_limit_exceeded(e, configurable.research_model) or True:
                # Token é™åˆ¶è¶…å‡ºæˆ–å…¶ä»–éŒ¯èª¤ - çµæŸç ”ç©¶éšæ®µ
                return Command(
                    goto=END,
                    update={
                        "notes": get_notes_from_tool_calls(supervisor_messages),
                        "research_brief": state.get("research_brief", "")
                    }
                )

    # æ­¥é©Ÿ 3ï¼šè¿”å›åŒ…å«æ‰€æœ‰å·¥å…·çµæœçš„å‘½ä»¤
    update_payload["supervisor_messages"] = all_tool_messages
    update_payload["timings"] = _append_timing(
        state, "supervisor_tools", _start_t)
    return Command(
        goto="supervisor",
        update=update_payload
    )

# ç›£ç£è€…å­åœ–æ§‹å»º
# å‰µå»ºç®¡ç†ç ”ç©¶å§”æ´¾å’Œå”èª¿çš„ç›£ç£è€…å·¥ä½œæµ
supervisor_builder = StateGraph(SupervisorState, config_schema=Configuration)

# ç‚ºç ”ç©¶ç®¡ç†æ·»åŠ ç›£ç£è€…ç¯€é»
supervisor_builder.add_node("supervisor", supervisor)           # ä¸»è¦ç›£ç£è€…é‚è¼¯
supervisor_builder.add_node("supervisor_tools", supervisor_tools)  # å·¥å…·åŸ·è¡Œè™•ç†å™¨

# å®šç¾©ç›£ç£è€…å·¥ä½œæµé‚Šç·£
supervisor_builder.add_edge(START, "supervisor")  # ç›£ç£è€…å…¥å£é»

# ç·¨è­¯ç›£ç£è€…å­åœ–ä»¥ä¾›ä¸»å·¥ä½œæµä½¿ç”¨
supervisor_subgraph = supervisor_builder.compile()


async def researcher(state: ResearcherState, config: RunnableConfig) -> Command[Literal["researcher_tools"]]:
    """å°ç‰¹å®šä¸»é¡Œé€²è¡Œå°ˆæ³¨ç ”ç©¶çš„å€‹åˆ¥ç ”ç©¶è€…ã€‚

    æ­¤ç ”ç©¶è€…ç”±ç›£ç£è€…çµ¦äºˆç‰¹å®šç ”ç©¶ä¸»é¡Œï¼Œä¸¦ä½¿ç”¨å¯ç”¨å·¥å…·ï¼ˆæœå°‹ã€think_toolã€MCP å·¥å…·ï¼‰
    æ”¶é›†å…¨é¢è³‡è¨Šã€‚å®ƒå¯ä»¥åœ¨æœå°‹ä¹‹é–“ä½¿ç”¨ think_tool é€²è¡Œæˆ°ç•¥è¦åŠƒã€‚

    Args:
        state: åŒ…å«è¨Šæ¯å’Œä¸»é¡Œä¸Šä¸‹æ–‡çš„ç•¶å‰ç ”ç©¶è€…ç‹€æ…‹
        config: åŒ…å«æ¨¡å‹è¨­å®šå’Œå·¥å…·å¯ç”¨æ€§çš„é‹è¡Œæ™‚é…ç½®

    Returns:
        é€²è¡Œ researcher_tools é€²è¡Œå·¥å…·åŸ·è¡Œçš„å‘½ä»¤
    """
    # print_progress("ğŸ” æ­£åœ¨æ”¶é›†ç ”ç©¶è³‡æ–™...")

    # æ­¥é©Ÿ 1ï¼šè¼‰å…¥é…ç½®ä¸¦é©—è­‰å·¥å…·å¯ç”¨æ€§
    _start_t = time.perf_counter()
    configurable = Configuration.from_runnable_config(config)
    researcher_messages = state.get("researcher_messages", [])
    # print_progress("âœ“ å·²è¼‰å…¥ç ”ç©¶è€…é…ç½®", 1)

    # ç²å–æ‰€æœ‰å¯ç”¨çš„ç ”ç©¶å·¥å…·ï¼ˆæœå°‹ã€MCPã€think_toolï¼‰
    # print_progress("âœ“ æ­£åœ¨è¼‰å…¥ç ”ç©¶å·¥å…·...", 1)
    tools = await get_all_tools(config)
    if len(tools) == 0:
        # print_progress("âœ— æœªæ‰¾åˆ°å¯ç”¨çš„ç ”ç©¶å·¥å…·", 1)
        raise ValueError(
            "No tools found to conduct research: Please configure either your "
            "search API or add MCP tools to your configuration."
        )
    # print_progress(f"âœ“ å·²è¼‰å…¥ {len(tools)} å€‹ç ”ç©¶å·¥å…·", 1)

    # æ­¥é©Ÿ 2ï¼šä½¿ç”¨å·¥å…·é…ç½®ç ”ç©¶è€…æ¨¡å‹
    # print_progress("âœ“ æ­£åœ¨é…ç½®ç ”ç©¶è€…æ¨¡å‹...", 1)
    research_model_config = {
        "model": configurable.research_model,
        "max_tokens": configurable.research_model_max_tokens,
        "tags": ["langsmith:nostream"]
    }
    research_model_config = {
        "model": configurable.research_model,
        "max_tokens": configurable.research_model_max_tokens,
        "tags": ["langsmith:nostream"]
    }

    # æº–å‚™ç³»çµ±æç¤ºï¼Œå¦‚æœå¯ç”¨å‰‡åŒ…å« MCP ä¸Šä¸‹æ–‡
    # print_progress("âœ“ æ­£åœ¨æº–å‚™ç ”ç©¶ç³»çµ±æç¤º...", 1)
    researcher_prompt = research_system_prompt.format(
        mcp_prompt=configurable.mcp_prompt or "",
        date=get_today_str()
    )

    # é…ç½®å…·æœ‰å·¥å…·ã€é‡è©¦é‚è¼¯å’Œè¨­å®šçš„æ¨¡å‹
    # print_progress("âœ“ æ­£åœ¨ç¶å®šå·¥å…·ä¸¦è¨­ç½®é‡è©¦é‚è¼¯...", 1)
    research_model = (
        configurable_model
        .bind_tools(tools)
        .with_retry(stop_after_attempt=configurable.max_structured_output_retries)
        .with_config(research_model_config)
    )

    # æ­¥é©Ÿ 3ï¼šä½¿ç”¨ç³»çµ±ä¸Šä¸‹æ–‡ç”Ÿæˆç ”ç©¶è€…å›æ‡‰
    # print_progress("âœ“ æ­£åœ¨åˆ†æç ”ç©¶ä¸»é¡Œä¸¦åˆ¶å®šæœå°‹ç­–ç•¥...", 1)
    messages = [SystemMessage(content=researcher_prompt)] + researcher_messages
    response = await research_model.ainvoke(messages)
    # print_progress("âœ“ ç ”ç©¶ç­–ç•¥å·²åˆ¶å®šï¼Œæº–å‚™åŸ·è¡Œå·¥å…·", 1)

    # æ­¥é©Ÿ 4ï¼šæ›´æ–°ç‹€æ…‹ä¸¦é€²è¡Œå·¥å…·åŸ·è¡Œ
    return Command(
        goto="researcher_tools",
        update={
            "researcher_messages": [response],
            "tool_call_iterations": state.get("tool_call_iterations", 0) + 1,
            "timings": _append_timing(state, "researcher", _start_t)
        }
    )

# å·¥å…·åŸ·è¡Œè¼”åŠ©å‡½æ•¸


async def execute_tool_safely(tool, args, config):
    """å®‰å…¨åŸ·è¡Œå·¥å…·ä¸¦è™•ç†éŒ¯èª¤ã€‚"""
    try:
        return await tool.ainvoke(args, config)
    except Exception as e:
        return f"Error executing tool: {str(e)}"


async def researcher_tools(state: ResearcherState, config: RunnableConfig) -> Command[Literal["researcher", "compress_research"]]:
    """åŸ·è¡Œç ”ç©¶è€…èª¿ç”¨çš„å·¥å…·ï¼ŒåŒ…æ‹¬æœå°‹å·¥å…·å’Œæˆ°ç•¥æ€è€ƒã€‚

    æ­¤å‡½æ•¸è™•ç†å„ç¨®é¡å‹çš„ç ”ç©¶è€…å·¥å…·èª¿ç”¨ï¼š
    1. think_tool - ç¹¼çºŒç ”ç©¶å°è©±çš„æˆ°ç•¥åæ€
    2. æœå°‹å·¥å…·ï¼ˆtavily_searchã€web_searchï¼‰- è³‡è¨Šæ”¶é›†
    3. MCP å·¥å…· - å¤–éƒ¨å·¥å…·æ•´åˆ
    4. ResearchComplete - ä¿¡è™Ÿå€‹åˆ¥ç ”ç©¶ä»»å‹™å®Œæˆ

    Args:
        state: åŒ…å«è¨Šæ¯å’Œè¿­ä»£è¨ˆæ•¸çš„ç•¶å‰ç ”ç©¶è€…ç‹€æ…‹
        config: åŒ…å«ç ”ç©¶é™åˆ¶å’Œå·¥å…·è¨­å®šçš„é‹è¡Œæ™‚é…ç½®

    Returns:
        ç¹¼çºŒç ”ç©¶å¾ªç’°æˆ–é€²è¡Œå£“ç¸®çš„å‘½ä»¤
    """
    # æ­¥é©Ÿ 1ï¼šæå–ç•¶å‰ç‹€æ…‹ä¸¦æª¢æŸ¥æ—©æœŸé€€å‡ºæ¢ä»¶
    _start_t = time.perf_counter()
    configurable = Configuration.from_runnable_config(config)
    researcher_messages = state.get("researcher_messages", [])
    most_recent_message = researcher_messages[-1]

    # å¦‚æœæ²’æœ‰é€²è¡Œå·¥å…·èª¿ç”¨å‰‡æ—©æœŸé€€å‡ºï¼ˆåŒ…æ‹¬åŸç”Ÿç¶²é æœå°‹ï¼‰
    has_tool_calls = bool(most_recent_message.tool_calls)
    has_native_search = (
        openai_websearch_called(most_recent_message) or
        anthropic_websearch_called(most_recent_message)
    )

    if not has_tool_calls and not has_native_search:
        return Command(goto="compress_research", update={"timings": _append_timing(state, "researcher_tools", _start_t)})

    # æ­¥é©Ÿ 2ï¼šè™•ç†å…¶ä»–å·¥å…·èª¿ç”¨ï¼ˆæœå°‹ã€MCP å·¥å…·ç­‰ï¼‰
    tools = await get_all_tools(config)
    tools_by_name = {
        tool.name if hasattr(tool, "name") else tool.get("name", "web_search"): tool
        for tool in tools
    }

    # ä¸¦è¡ŒåŸ·è¡Œæ‰€æœ‰å·¥å…·èª¿ç”¨
    tool_calls = most_recent_message.tool_calls
    tool_execution_tasks = [
        execute_tool_safely(
            tools_by_name[tool_call["name"]], tool_call["args"], config)
        for tool_call in tool_calls
    ]
    observations = await asyncio.gather(*tool_execution_tasks)

    # å¾åŸ·è¡Œçµæœå‰µå»ºå·¥å…·è¨Šæ¯
    tool_outputs = [
        ToolMessage(
            content=observation,
            name=tool_call["name"],
            tool_call_id=tool_call["id"]
        )
        for observation, tool_call in zip(observations, tool_calls)
    ]

    # æ­¥é©Ÿ 3ï¼šæª¢æŸ¥å¾ŒæœŸé€€å‡ºæ¢ä»¶ï¼ˆè™•ç†å·¥å…·å¾Œï¼‰
    exceeded_iterations = state.get(
        "tool_call_iterations", 0) >= configurable.max_react_tool_calls
    research_complete_called = any(
        tool_call["name"] == "ResearchComplete"
        for tool_call in most_recent_message.tool_calls
    )

    if exceeded_iterations or research_complete_called:
        # çµæŸç ”ç©¶ä¸¦é€²è¡Œå£“ç¸®
        return Command(
            goto="compress_research",
            update={
                "researcher_messages": tool_outputs,
                "timings": _append_timing(state, "researcher_tools", _start_t)
            }
        )

    # ä½¿ç”¨å·¥å…·çµæœç¹¼çºŒç ”ç©¶å¾ªç’°
    return Command(
        goto="researcher",
        update={
            "researcher_messages": tool_outputs,
            "timings": _append_timing(state, "researcher_tools", _start_t)
        }
    )


async def compress_research(state: ResearcherState, config: RunnableConfig):
    """å°‡ç ”ç©¶ç™¼ç¾å£“ç¸®ä¸¦åˆæˆç‚ºç°¡æ½”ã€çµæ§‹åŒ–çš„æ‘˜è¦ã€‚

    æ­¤å‡½æ•¸ç²å–ç ”ç©¶è€…çš„æ‰€æœ‰ç ”ç©¶ç™¼ç¾ã€å·¥å…·è¼¸å‡ºå’Œ AI è¨Šæ¯ï¼Œä¸¦å°‡å®ƒå€‘è’¸é¤¾æˆ
    æ¸…æ½”ã€å…¨é¢çš„æ‘˜è¦ï¼ŒåŒæ™‚ä¿ç•™æ‰€æœ‰é‡è¦è³‡è¨Šå’Œç™¼ç¾ã€‚

    Args:
        state: åŒ…å«ç´¯ç©ç ”ç©¶è¨Šæ¯çš„ç•¶å‰ç ”ç©¶è€…ç‹€æ…‹
        config: åŒ…å«å£“ç¸®æ¨¡å‹è¨­å®šçš„é‹è¡Œæ™‚é…ç½®

    Returns:
        åŒ…å«å£“ç¸®ç ”ç©¶æ‘˜è¦å’ŒåŸå§‹ç­†è¨˜çš„å­—å…¸
    """
    # print_progress("ğŸ“Š æ­£åœ¨æ•´ç†ç ”ç©¶çµæœ...")

    # æ­¥é©Ÿ 1ï¼šé…ç½®å£“ç¸®æ¨¡å‹
    _start_t = time.perf_counter()
    configurable = Configuration.from_runnable_config(config)
    # print_progress("âœ“ å·²è¼‰å…¥å£“ç¸®é…ç½®", 1)

    # print_progress("âœ“ æ­£åœ¨é…ç½®å£“ç¸®æ¨¡å‹...", 1)
    synthesizer_model = configurable_model.with_config({
        "model": configurable.compression_model,
        "max_tokens": configurable.compression_model_max_tokens,
        "tags": ["langsmith:nostream"]
    })

    # æ­¥é©Ÿ 2ï¼šç‚ºå£“ç¸®æº–å‚™è¨Šæ¯
    # print_progress("âœ“ æ­£åœ¨æº–å‚™ç ”ç©¶è³‡æ–™é€²è¡Œå£“ç¸®...", 1)
    researcher_messages = state.get("researcher_messages", [])

    # æ·»åŠ å¾ç ”ç©¶æ¨¡å¼åˆ‡æ›åˆ°å£“ç¸®æ¨¡å¼çš„æŒ‡ç¤º
    researcher_messages.append(HumanMessage(
        content=compress_research_simple_human_message))

    # æ­¥é©Ÿ 3ï¼šå˜—è©¦å£“ç¸®ï¼Œå° token é™åˆ¶å•é¡Œä½¿ç”¨é‡è©¦é‚è¼¯
    # print_progress("âœ“ æ­£åœ¨åŸ·è¡Œç ”ç©¶è³‡æ–™å£“ç¸®...", 1)
    synthesis_attempts = 0
    max_attempts = 3

    while synthesis_attempts < max_attempts:
        try:
            # å‰µå»ºå°ˆæ³¨æ–¼å£“ç¸®ä»»å‹™çš„ç³»çµ±æç¤º
            compression_prompt = compress_research_system_prompt.format(
                date=get_today_str())
            messages = [SystemMessage(
                content=compression_prompt)] + researcher_messages

            # åŸ·è¡Œå£“ç¸®
            response = await synthesizer_model.ainvoke(messages)

            # å¾æ‰€æœ‰å·¥å…·å’Œ AI è¨Šæ¯ä¸­æå–åŸå§‹ç­†è¨˜
            raw_notes_content = "\n".join([
                str(message.content)
                for message in filter_messages(researcher_messages, include_types=["tool", "ai"])
            ])

            # è¿”å›æˆåŠŸçš„å£“ç¸®çµæœ
            return {
                "compressed_research": str(response.content),
                "raw_notes": [raw_notes_content],
                "timings": _append_timing(state, "compress_research", _start_t)
            }

        except Exception as e:
            synthesis_attempts += 1

            # é€šéç§»é™¤è¼ƒèˆŠçš„è¨Šæ¯è™•ç† token é™åˆ¶è¶…å‡º
            if is_token_limit_exceeded(e, configurable.research_model):
                researcher_messages = remove_up_to_last_ai_message(
                    researcher_messages)
                continue

            # å°æ–¼å…¶ä»–éŒ¯èª¤ï¼Œç¹¼çºŒé‡è©¦
            continue

    # æ­¥é©Ÿ 4ï¼šå¦‚æœæ‰€æœ‰å˜—è©¦éƒ½å¤±æ•—å‰‡è¿”å›éŒ¯èª¤çµæœ
    raw_notes_content = "\n".join([
        str(message.content)
        for message in filter_messages(researcher_messages, include_types=["tool", "ai"])
    ])

    return {
        "compressed_research": "Error synthesizing research report: Maximum retries exceeded",
        "raw_notes": [raw_notes_content],
        "timings": _append_timing(state, "compress_research", _start_t)
    }

# ç ”ç©¶è€…å­åœ–æ§‹å»º
# å‰µå»ºå°ç‰¹å®šä¸»é¡Œé€²è¡Œå°ˆæ³¨ç ”ç©¶çš„å€‹åˆ¥ç ”ç©¶è€…å·¥ä½œæµ
researcher_builder = StateGraph(
    ResearcherState,
    output=ResearcherOutputState,
    config_schema=Configuration
)

# ç‚ºç ”ç©¶åŸ·è¡Œå’Œå£“ç¸®æ·»åŠ ç ”ç©¶è€…ç¯€é»
researcher_builder.add_node("researcher", researcher)                 # ä¸»è¦ç ”ç©¶è€…é‚è¼¯
researcher_builder.add_node("researcher_tools", researcher_tools)     # å·¥å…·åŸ·è¡Œè™•ç†å™¨
researcher_builder.add_node("compress_research", compress_research)   # ç ”ç©¶å£“ç¸®

# å®šç¾©ç ”ç©¶è€…å·¥ä½œæµé‚Šç·£
researcher_builder.add_edge(START, "researcher")           # ç ”ç©¶è€…å…¥å£é»
researcher_builder.add_edge("compress_research", END)      # å£“ç¸®å¾Œé€€å‡ºé»

# ç·¨è­¯ç ”ç©¶è€…å­åœ–ä»¥ä¾›ç›£ç£è€…ä¸¦è¡ŒåŸ·è¡Œ
researcher_subgraph = researcher_builder.compile()


async def final_report_generation(state: AgentState, config: RunnableConfig):
    """ä½¿ç”¨ token é™åˆ¶çš„é‡è©¦é‚è¼¯ç”Ÿæˆæœ€çµ‚çš„å…¨é¢ç ”ç©¶å ±å‘Šã€‚

    æ­¤å‡½æ•¸ç²å–æ‰€æœ‰æ”¶é›†çš„ç ”ç©¶ç™¼ç¾ï¼Œä¸¦ä½¿ç”¨é…ç½®çš„å ±å‘Šç”Ÿæˆæ¨¡å‹å°‡å®ƒå€‘åˆæˆç‚º
    çµæ§‹è‰¯å¥½çš„å…¨é¢æœ€çµ‚å ±å‘Šã€‚

    Args:
        state: åŒ…å«ç ”ç©¶ç™¼ç¾å’Œä¸Šä¸‹æ–‡çš„ä»£ç†ç‹€æ…‹
        config: åŒ…å«æ¨¡å‹è¨­å®šå’Œ API é‡‘é‘°çš„é‹è¡Œæ™‚é…ç½®

    Returns:
        åŒ…å«æœ€çµ‚å ±å‘Šå’Œæ¸…é™¤ç‹€æ…‹çš„å­—å…¸
    """
    # æ¨™æº–åŒ–éšæ®µäº‹ä»¶ï¼šå ±å‘Šç”Ÿæˆéšæ®µ
    print_progress("STAGE::final_report_generation::enter")
    # print_progress("ğŸ“„ æ­£åœ¨æ’°å¯«æœ€çµ‚å ±å‘Š...")

    # æ­¥é©Ÿ 1ï¼šæå–ç ”ç©¶ç™¼ç¾ä¸¦æº–å‚™ç‹€æ…‹æ¸…ç†
    notes = state.get("notes", [])
    cleared_state = {"notes": {"type": "override", "value": []}}
    findings = "\n".join(notes)

    # print_progress(f"âœ“ æ­£åœ¨æ•´åˆ {len(notes)} é …ç ”ç©¶ç™¼ç¾...", 1)

    # æ­¥é©Ÿ 2ï¼šé…ç½®æœ€çµ‚å ±å‘Šç”Ÿæˆæ¨¡å‹
    _start_t = time.perf_counter()
    configurable = Configuration.from_runnable_config(config)
    # print_progress("âœ“ å·²è¼‰å…¥å ±å‘Šç”Ÿæˆé…ç½®", 1)

    writer_model_config = {
        "model": configurable.final_report_model,
        "max_tokens": configurable.final_report_model_max_tokens,
        "tags": ["langsmith:nostream"]
    }

    # æ­¥é©Ÿ 3ï¼šä½¿ç”¨ token é™åˆ¶é‡è©¦é‚è¼¯å˜—è©¦å ±å‘Šç”Ÿæˆ
    # print_progress("âœ“ æ­£åœ¨é…ç½®å ±å‘Šç”Ÿæˆæ¨¡å‹...", 1)
    max_retries = 3
    current_retry = 0
    findings_token_limit = None

    while current_retry <= max_retries:
        try:
            # å‰µå»ºåŒ…å«æ‰€æœ‰ç ”ç©¶ä¸Šä¸‹æ–‡çš„å…¨é¢æç¤º
            final_report_prompt = final_report_generation_prompt.format(
                research_brief=state.get("research_brief", ""),
                messages=get_buffer_string(state.get("messages", [])),
                findings=findings,
                date=get_today_str()
            )

            # ç”Ÿæˆæœ€çµ‚å ±å‘Š
            final_report = await configurable_model.with_config(writer_model_config).ainvoke([
                HumanMessage(content=final_report_prompt)
            ])

            # è¿”å›æˆåŠŸçš„å ±å‘Šç”Ÿæˆ
            return {
                "final_report": final_report.content,
                "messages": [final_report],
                "timings": _append_timing(state, "final_report_generation", _start_t),
                **cleared_state
            }

        except Exception as e:
            # ä½¿ç”¨æ¼¸é€²å¼æˆªæ–·è™•ç† token é™åˆ¶è¶…å‡ºéŒ¯èª¤
            if is_token_limit_exceeded(e, configurable.final_report_model):
                current_retry += 1

                if current_retry == 1:
                    # ç¬¬ä¸€æ¬¡é‡è©¦ï¼šç¢ºå®šåˆå§‹æˆªæ–·é™åˆ¶
                    model_token_limit = get_model_token_limit(
                        configurable.final_report_model)
                    if not model_token_limit:
                        return {
                            "final_report": f"Error generating final report: Token limit exceeded, however, we could not determine the model's maximum context length. Please update the model map in deep_researcher/utils.py with this information. {e}",
                            "messages": [AIMessage(content="Report generation failed due to token limits")],
                            **cleared_state
                        }
                    # ä½¿ç”¨ 4 å€ token é™åˆ¶ä½œç‚ºæˆªæ–·çš„å­—ç¬¦è¿‘ä¼¼å€¼
                    findings_token_limit = model_token_limit * 4
                else:
                    # å¾ŒçºŒé‡è©¦ï¼šæ¯æ¬¡æ¸›å°‘ 10%
                    findings_token_limit = int(findings_token_limit * 0.9)

                # æˆªæ–·ç™¼ç¾ä¸¦é‡è©¦
                findings = findings[:findings_token_limit]
                continue
            else:
                # é token é™åˆ¶éŒ¯èª¤ï¼šç«‹å³è¿”å›éŒ¯èª¤
                return {
                    "final_report": f"Error generating final report: {e}",
                    "messages": [AIMessage(content="Report generation failed due to an error")],
                    "timings": _append_timing(state, "final_report_generation", _start_t),
                    **cleared_state
                }

    # æ­¥é©Ÿ 4ï¼šå¦‚æœæ‰€æœ‰é‡è©¦éƒ½ç”¨ç›¡å‰‡è¿”å›å¤±æ•—çµæœ
    return {
        "final_report": "Error generating final report: Maximum retries exceeded",
        "messages": [AIMessage(content="Report generation failed after maximum retries")],
        "timings": _append_timing(state, "final_report_generation", _start_t),
        **cleared_state
    }

# ä¸»è¦æ·±åº¦ç ”ç©¶è€…åœ–æ§‹å»º
# å‰µå»ºå¾ä½¿ç”¨è€…è¼¸å…¥åˆ°æœ€çµ‚å ±å‘Šçš„å®Œæ•´æ·±åº¦ç ”ç©¶å·¥ä½œæµ
deep_researcher_builder = StateGraph(
    AgentState,
    input=AgentInputState,
    config_schema=Configuration
)

# ç‚ºå®Œæ•´ç ”ç©¶éç¨‹æ·»åŠ ä¸»è¦å·¥ä½œæµç¯€é»
deep_researcher_builder.add_node(
    "clarify_with_user", clarify_with_user)           # ä½¿ç”¨è€…é‡æ¸…éšæ®µ
deep_researcher_builder.add_node(
    "write_research_brief", write_research_brief)     # ç ”ç©¶è¦åŠƒéšæ®µ
deep_researcher_builder.add_node(
    "research_supervisor", supervisor_subgraph)       # ç ”ç©¶åŸ·è¡Œéšæ®µ
deep_researcher_builder.add_node(
    "final_report_generation", final_report_generation)  # å ±å‘Šç”Ÿæˆéšæ®µ

# å®šç¾©é †åºåŸ·è¡Œçš„ä¸»è¦å·¥ä½œæµé‚Šç·£
deep_researcher_builder.add_edge(
    START, "clarify_with_user")                       # å…¥å£é»
deep_researcher_builder.add_edge(
    "research_supervisor", "final_report_generation")  # ç ”ç©¶åˆ°å ±å‘Š
deep_researcher_builder.add_edge(
    "final_report_generation", END)                   # æœ€çµ‚é€€å‡ºé»

# ç·¨è­¯å®Œæ•´çš„æ·±åº¦ç ”ç©¶è€…å·¥ä½œæµ
deep_researcher = deep_researcher_builder.compile()
